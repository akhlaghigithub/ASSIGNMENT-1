# -*- coding: utf-8 -*-
"""classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ccru59eC4MatnkFcHLC_55egMcOYLWqr
"""

!pip install wikipedia-api

import wikipediaapi
import spacy
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

# Function to fetch Wikipedia page text with a specified user-agent
def fetch_wikipedia_page(title):
    user_agent = "your-unique-user-agent"
    headers = {
        'User-Agent': user_agent
    }
    wiki_wiki = wikipediaapi.Wikipedia('en', headers=headers)
    page = wiki_wiki.page(title)
    return page.text

# Collecting sample data
geo_titles = ["Paris", "New York City", "Mount Everest", "Amazon River"]
non_geo_titles = ["Albert Einstein", "Photosynthesis", "Quantum Mechanics", "Shakespeare"]

geo_texts = [fetch_wikipedia_page(title) for title in geo_titles]
non_geo_texts = [fetch_wikipedia_page(title) for title in non_geo_titles]

texts = geo_texts + non_geo_texts
labels = ["geographic"] * len(geo_texts) + ["non-geographic"] * len(non_geo_texts)

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)

# Creating the model using TfidfVectorizer and Naive Bayes
model = make_pipeline(TfidfVectorizer(), MultinomialNB())

# Training the model
model.fit(X_train, y_train)

# Predicting with the model
y_pred = model.predict(X_test)

# Displaying the evaluation report
print(classification_report(y_test, y_pred))

# Function to classify new text
def classify_text(text):
    return model.predict([text])[0]

# Example of classifying a new text
new_text = "The Amazon River in South America is the largest river by discharge volume of water in the world."
print(f"The text is classified as: {classify_text(new_text)}")